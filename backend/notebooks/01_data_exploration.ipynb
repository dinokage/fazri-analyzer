{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9714f6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Campus Entity Resolution - Dataset Analysis\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set styling\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Data paths\n",
    "DATA_DIR = Path(\"/Users/dinokage/dev/fazri-analyzer/backend/augmented\")\n",
    "\n",
    "print(\"üìä Campus Entity Resolution - Dataset Analysis\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45989325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded profiles: 7000 rows\n",
      "‚úÖ Loaded cctv: 28000 rows\n",
      "‚úÖ Loaded swipes: 32000 rows\n",
      "‚úÖ Loaded wifi: 32000 rows\n",
      "‚úÖ Loaded library: 28000 rows\n",
      "‚úÖ Loaded bookings: 28000 rows\n",
      "‚úÖ Loaded helpdesk: 28000 rows\n",
      "‚úÖ Loaded face_embeddings: 6973 rows\n"
     ]
    }
   ],
   "source": [
    "def load_datasets():\n",
    "    \"\"\"Load all CSV datasets and return dict\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    csv_files = {\n",
    "        'profiles': 'student_staff_profiles.csv',\n",
    "        'cctv': 'cctv_frames_augmented.csv',\n",
    "        'swipes': 'campus_card_swipes_augmented.csv', \n",
    "        'wifi': 'wifi_associations_logs_augmented.csv',\n",
    "        'library': 'library_checkouts_augmented.csv',\n",
    "        'bookings': 'lab_bookings_augmented.csv',\n",
    "        'helpdesk': 'helpdesk_augmented.csv',\n",
    "        'face_embeddings': 'face_embeddings.csv'\n",
    "    }\n",
    "    \n",
    "    for name, filename in csv_files.items():\n",
    "        filepath = DATA_DIR / filename\n",
    "        if filepath.exists():\n",
    "            datasets[name] = pd.read_csv(filepath)\n",
    "            print(f\"‚úÖ Loaded {name}: {len(datasets[name])} rows\")\n",
    "        else:\n",
    "            print(f\"‚ùå Missing {name}: {filename}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Load all data\n",
    "data = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e0d35e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç STUDENT/STAFF PROFILES ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Total Records: 7000\n",
      "\n",
      "Columns: ['entity_id', 'name', 'role', 'email', 'department', 'student_id', 'staff_id', 'card_id', 'device_hash', 'face_id']\n",
      "\n",
      "Data Types:\n",
      "entity_id      object\n",
      "name           object\n",
      "role           object\n",
      "email          object\n",
      "department     object\n",
      "student_id     object\n",
      "staff_id       object\n",
      "card_id        object\n",
      "device_hash    object\n",
      "face_id        object\n",
      "dtype: object\n",
      "\n",
      "First 5 Records:\n",
      "    entity_id          name     role               email department  \\\n",
      "0     E100000    Neha Mehta  student    user0@campus.edu      CIVIL   \n",
      "1     E100001    Neha Kumar  student    user1@campus.edu    Physics   \n",
      "2     E100002    Neha Singh  student    user2@campus.edu      Admin   \n",
      "3     E100003  Ishaan Desai  student    user3@campus.edu      Admin   \n",
      "4     E100004   Rohan Desai  student    user4@campus.edu       MECH   \n",
      "..        ...           ...      ...                 ...        ...   \n",
      "195   E100195  Rohan Sharma  student  user195@campus.edu       MECH   \n",
      "196   E100196   Rohan Mehta  student  user196@campus.edu      CIVIL   \n",
      "197   E100197  Aarav Sharma  student  user197@campus.edu       MECH   \n",
      "198   E100198      Neha Rao  student  user198@campus.edu        BIO   \n",
      "199   E100199   Divya Kumar  student  user199@campus.edu      Admin   \n",
      "\n",
      "    student_id staff_id card_id     device_hash  face_id  \n",
      "0       S39256      NaN   C3286  DH6d0bd80c8f8e  F100000  \n",
      "1       S14165      NaN   C1488  DH75af13047587  F100001  \n",
      "2       S13478      NaN   C4257  DH7a5f80cb6e8d  F100002  \n",
      "3       S46463      NaN   C1106  DH3f7f51a6e78d  F100003  \n",
      "4       S38221      NaN   C6514  DH64dfe3d9327e  F100004  \n",
      "..         ...      ...     ...             ...      ...  \n",
      "195     S24408      NaN   C1422  DH3570320d5fd5  F100195  \n",
      "196     S87182      NaN   C6304  DH98c2bf5639bd  F100196  \n",
      "197     S65176      NaN   C9619  DHc0dc4e421e66  F100197  \n",
      "198     S77271      NaN   C2786  DH100b42cd0990  F100198  \n",
      "199     S15683      NaN   C9543  DH69e51d432dc1  F100199  \n",
      "\n",
      "[200 rows x 10 columns]\n",
      "\n",
      "üìã Identifier Columns: ['entity_id', 'email', 'student_id', 'staff_id', 'card_id', 'device_hash', 'face_id']\n",
      "\n",
      "‚ö†Ô∏è Missing Values in Identifiers:\n",
      "  - entity_id: 0 (0.0%)\n",
      "  - email: 0 (0.0%)\n",
      "  - student_id: 1399 (20.0%)\n",
      "  - staff_id: 5601 (80.0%)\n",
      "  - card_id: 0 (0.0%)\n",
      "  - device_hash: 0 (0.0%)\n",
      "  - face_id: 2000 (28.6%)\n",
      "\n",
      "üîÑ Duplicate Analysis:\n",
      "  - entity_id: 0 duplicates\n",
      "  - email: 0 duplicates\n",
      "  - student_id: 1559 duplicates\n",
      "  - staff_id: 5714 duplicates\n",
      "  - card_id: 2140 duplicates\n",
      "  - device_hash: 0 duplicates\n",
      "  - face_id: 1999 duplicates\n",
      "\n",
      "üìù Name Variations:\n",
      "  - Unique names: 100\n",
      "  - Names appearing >1 time: 100\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Profile Data Analysis\n",
    "print(\"\\nüîç STUDENT/STAFF PROFILES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'profiles' in data:\n",
    "    df = data['profiles']\n",
    "    \n",
    "    print(f\"\\nTotal Records: {len(df)}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nData Types:\\n{df.dtypes}\")\n",
    "    print(f\"\\nFirst 5 Records:\\n{df.head(200)}\")\n",
    "    \n",
    "    # Identify all identifier columns\n",
    "    id_columns = [col for col in df.columns if 'id' in col.lower() \n",
    "                  or col.lower() in ['email', 'card_id', 'device_hash', 'face_id', 'entity_id']]\n",
    "    print(f\"\\nüìã Identifier Columns: {id_columns}\")\n",
    "    \n",
    "    # Check for missing values in identifiers\n",
    "    print(f\"\\n‚ö†Ô∏è Missing Values in Identifiers:\")\n",
    "    for col in id_columns:\n",
    "        if col in df.columns:\n",
    "            missing = df[col].isna().sum()\n",
    "            missing_pct = (missing / len(df)) * 100\n",
    "            print(f\"  - {col}: {missing} ({missing_pct:.1f}%)\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    print(f\"\\nüîÑ Duplicate Analysis:\")\n",
    "    for col in id_columns:\n",
    "        if col in df.columns:\n",
    "            duplicates = df[col].duplicated().sum()\n",
    "            print(f\"  - {col}: {duplicates} duplicates\")\n",
    "    \n",
    "    # Name variations analysis\n",
    "    if 'name' in df.columns:\n",
    "        print(f\"\\nüìù Name Variations:\")\n",
    "        name_counts = df['name'].value_counts()\n",
    "        print(f\"  - Unique names: {len(name_counts)}\")\n",
    "        print(f\"  - Names appearing >1 time: {(name_counts > 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0428069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Card Swipe Analysis\n",
    "print(\"\\nüîç CARD SWIPE LOGS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'swipes' in data:\n",
    "    df = data['swipes']\n",
    "    \n",
    "    print(f\"\\nTotal Swipes: {len(df)}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nSample Data:\\n{df.head(20)}\")\n",
    "    \n",
    "    # Convert timestamp\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Temporal analysis\n",
    "    print(f\"\\nüìÖ Temporal Coverage:\")\n",
    "    print(f\"  - Start: {df['timestamp'].min()}\")\n",
    "    print(f\"  - End: {df['timestamp'].max()}\")\n",
    "    print(f\"  - Duration: {(df['timestamp'].max() - df['timestamp'].min()).days} days\")\n",
    "    \n",
    "    # Location analysis\n",
    "    if 'location_id' in df.columns:\n",
    "        print(f\"\\nüìç Location Distribution:\")\n",
    "        print(df['location_id'].value_counts().head(10))\n",
    "    \n",
    "    # Unique entities\n",
    "    if 'card_id' in df.columns:\n",
    "        print(f\"\\nüë• Unique Cards: {df['card_id'].nunique()}\")\n",
    "        \n",
    "    # Activity patterns\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    print(f\"\\n‚è∞ Peak Activity Hours:\")\n",
    "    print(df['hour'].value_counts().sort_index().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2aaf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Wi-Fi Logs Analysis\n",
    "print(\"\\nüîç WI-FI LOGS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'wifi' in data:\n",
    "    df = data['wifi']\n",
    "    \n",
    "    print(f\"\\nTotal Wi-Fi Connections: {len(df)}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nSample Data:\\n{df.head(20)}\")\n",
    "    \n",
    "    # Convert timestamp\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Device analysis\n",
    "    if 'device_hash' in df.columns:\n",
    "        print(f\"\\nüì± Unique Devices: {df['device_hash'].nunique()}\")\n",
    "    \n",
    "    # Access point analysis\n",
    "    if 'ap_id' in df.columns:\n",
    "        print(f\"\\nüì° Access Points:\")\n",
    "        print(df['ap_id'].value_counts().head(10))\n",
    "    \n",
    "    # Temporal patterns\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    print(f\"\\n‚è∞ Connection Patterns by Hour:\")\n",
    "    hourly = df.groupby('hour').size()\n",
    "    print(hourly.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Library Checkouts Analysis\n",
    "print(\"\\nüîç LIBRARY CHECKOUTS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'library' in data:\n",
    "    df = data['library']\n",
    "    \n",
    "    print(f\"\\nTotal Checkouts: {len(df)}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nSample Data:\\n{df.head(20)}\")\n",
    "    \n",
    "    # Identifier analysis\n",
    "    id_cols = [col for col in df.columns if 'id' in col.lower() or col.lower() in ['timestamp']]\n",
    "    print(f\"\\nüÜî Identifiers: {id_cols}\")\n",
    "    \n",
    "    # Most borrowed items\n",
    "    if 'item_id' in df.columns or 'book_id' in df.columns:\n",
    "        item_col = 'item_id' if 'item_id' in df.columns else 'book_id'\n",
    "        print(f\"\\nüìö Most Borrowed Items:\")\n",
    "        print(df[item_col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Helpdesk Notes Analysis\n",
    "print(\"\\nüîç HELPDESK NOTES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'helpdesk' in data:\n",
    "    df = data['helpdesk']\n",
    "    \n",
    "    print(f\"\\nTotal Tickets: {len(df)}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nSample Data:\\n{df.head(2000)}\")\n",
    "    \n",
    "    # Text analysis\n",
    "    if 'notes' in df.columns or 'description' in df.columns:\n",
    "        text_col = 'notes' if 'notes' in df.columns else 'description'\n",
    "        print(f\"\\nüìù Text Statistics:\")\n",
    "        print(f\"  - Avg length: {df[text_col].str.len().mean():.0f} chars\")\n",
    "        print(f\"  - Max length: {df[text_col].str.len().max()} chars\")\n",
    "        \n",
    "        # Sample text\n",
    "        print(f\"\\nüìÑ Sample Text:\")\n",
    "        print(df[text_col].iloc[0][:200] + \"...\")\n",
    "    \n",
    "    # Check for embedded identifiers in text\n",
    "    print(f\"\\nüîç Checking for identifiers in text...\")\n",
    "    if 'notes' in df.columns or 'description' in df.columns:\n",
    "        text_col = 'notes' if 'notes' in df.columns else 'description'\n",
    "        \n",
    "        # Look for patterns\n",
    "        import re\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        id_pattern = r'\\b[A-Z]{2,}\\d{4,}\\b'\n",
    "        \n",
    "        emails = df[text_col].str.extract(f'({email_pattern})', expand=False).dropna()\n",
    "        ids = df[text_col].str.extract(f'({id_pattern})', expand=False).dropna()\n",
    "        \n",
    "        print(f\"  - Found {len(emails)} potential emails\")\n",
    "        print(f\"  - Found {len(ids)} potential IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bbef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Face Embeddings Analysis\n",
    "print(\"\\nüîç FACE EMBEDDINGS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'face_embeddings' in data:\n",
    "    df = data['face_embeddings']\n",
    "    \n",
    "    print(f\"\\nTotal Embeddings: {len(df)}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nSample Data:\\n{df.head(20)}\")\n",
    "    \n",
    "    # Check embedding dimensions\n",
    "    embedding_cols = [col for col in df.columns if 'embed' in col.lower() or col.startswith('dim_')]\n",
    "    print(f\"\\nüéØ Embedding Dimensions: {len(embedding_cols)}\")\n",
    "    \n",
    "    # Unique faces\n",
    "    if 'face_id' in df.columns or 'person_id' in df.columns:\n",
    "        id_col = 'face_id' if 'face_id' in df.columns else 'person_id'\n",
    "        print(f\"\\nüë§ Unique Faces: {df[id_col].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Cross-Dataset Identifier Mapping\n",
    "print(\"\\nüîç CROSS-DATASET IDENTIFIER MAPPING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build identifier co-occurrence matrix\n",
    "identifier_types = {\n",
    "    'student_id': [],\n",
    "    'entity_id': [],\n",
    "    'staff_id': [],\n",
    "    'email': [],\n",
    "    'card_id': [],\n",
    "    'device_hash': [],\n",
    "    'face_id': []\n",
    "}\n",
    "\n",
    "# Check which datasets have which identifiers\n",
    "for dataset_name, df in data.items():\n",
    "    print(f\"\\n{dataset_name.upper()}:\")\n",
    "    for id_type in identifier_types.keys():\n",
    "        if id_type in df.columns:\n",
    "            count = df[id_type].notna().sum()\n",
    "            unique = df[id_type].nunique()\n",
    "            print(f\"  ‚úÖ {id_type}: {count} records, {unique} unique\")\n",
    "            identifier_types[id_type].append(dataset_name)\n",
    "        else:\n",
    "            print(f\"  ‚ùå {id_type}: not present\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nüìä IDENTIFIER AVAILABILITY SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "for id_type, datasets in identifier_types.items():\n",
    "    if datasets:\n",
    "        print(f\"{id_type}: Found in {len(datasets)} datasets - {', '.join(datasets)}\")\n",
    "    else:\n",
    "        print(f\"{id_type}: Not found in any dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526bda69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Data Quality Assessment\n",
    "print(\"\\nüîç DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "quality_report = {}\n",
    "\n",
    "for name, df in data.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isna().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    \n",
    "    if missing.sum() > 0:\n",
    "        print(f\"  ‚ö†Ô∏è Columns with missing data:\")\n",
    "        for col, pct in missing_pct[missing_pct > 0].items():\n",
    "            print(f\"    - {col}: {pct:.1f}%\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ No missing data\")\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"  ‚ö†Ô∏è Duplicate rows: {duplicates}\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ No duplicate rows\")\n",
    "    \n",
    "    quality_report[name] = {\n",
    "        'total_rows': len(df),\n",
    "        'missing_values': missing.sum(),\n",
    "        'duplicates': duplicates\n",
    "    }\n",
    "\n",
    "# Overall quality score\n",
    "print(f\"\\nüìä OVERALL DATA QUALITY:\")\n",
    "total_rows = sum(r['total_rows'] for r in quality_report.values())\n",
    "total_missing = sum(r['missing_values'] for r in quality_report.values())\n",
    "total_duplicates = sum(r['duplicates'] for r in quality_report.values())\n",
    "\n",
    "completeness = (1 - total_missing / (total_rows * len(data))) * 100\n",
    "print(f\"  - Completeness: {completeness:.1f}%\")\n",
    "print(f\"  - Total duplicates: {total_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Generate Analysis Summary Report\n",
    "print(\"\\nüìù GENERATING ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Helper function to convert numpy/pandas types to native Python types\n",
    "def convert_to_json_serializable(obj):\n",
    "    \"\"\"Convert numpy/pandas types to JSON-serializable Python types\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_json_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_json_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif pd.isna(obj):\n",
    "        return None\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "report = {\n",
    "    'dataset_overview': {\n",
    "        'total_datasets': len(data),\n",
    "        'total_records': sum(len(df) for df in data.values()),\n",
    "        'datasets': {name: len(df) for name, df in data.items()}\n",
    "    },\n",
    "    'identifiers': {},\n",
    "    'temporal_coverage': {},\n",
    "    'data_quality': {}\n",
    "}\n",
    "\n",
    "# Convert quality report to JSON-serializable format\n",
    "for name, stats in quality_report.items():\n",
    "    report['data_quality'][name] = {\n",
    "        'total_rows': int(stats['total_rows']),\n",
    "        'missing_values': int(stats['missing_values']),\n",
    "        'duplicates': int(stats['duplicates'])\n",
    "    }\n",
    "\n",
    "# Identifier summary\n",
    "for id_type, datasets in identifier_types.items():\n",
    "    if datasets:\n",
    "        report['identifiers'][id_type] = {\n",
    "            'available_in': datasets,\n",
    "            'count': len(datasets)\n",
    "        }\n",
    "\n",
    "# Add temporal coverage if available\n",
    "if 'swipes' in data:\n",
    "    df = data['swipes'].copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    report['temporal_coverage']['swipes'] = {\n",
    "        'start_date': df['timestamp'].min().isoformat(),\n",
    "        'end_date': df['timestamp'].max().isoformat(),\n",
    "        'duration_days': int((df['timestamp'].max() - df['timestamp'].min()).days)\n",
    "    }\n",
    "\n",
    "if 'wifi' in data:\n",
    "    df = data['wifi'].copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    report['temporal_coverage']['wifi'] = {\n",
    "        'start_date': df['timestamp'].min().isoformat(),\n",
    "        'end_date': df['timestamp'].max().isoformat(),\n",
    "        'duration_days': int((df['timestamp'].max() - df['timestamp'].min()).days)\n",
    "    }\n",
    "\n",
    "# Convert entire report to ensure all values are JSON serializable\n",
    "report = convert_to_json_serializable(report)\n",
    "\n",
    "# Save report\n",
    "import json\n",
    "report_path = Path(\"../../data/analysis/dataset_analysis_report.json\")\n",
    "report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2, default=str)  # Added default=str as fallback\n",
    "\n",
    "print(f\"‚úÖ Report saved to: {report_path}\")\n",
    "print(f\"\\nüìä Analysis Summary:\")\n",
    "print(json.dumps(report, indent=2))\n",
    "\n",
    "# Also save a human-readable markdown report\n",
    "md_report_path = Path(\"../../data/analysis/dataset_analysis_report.md\")\n",
    "\n",
    "with open(md_report_path, 'w') as f:\n",
    "    f.write(\"# Dataset Analysis Report\\n\\n\")\n",
    "    f.write(f\"**Generated:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Dataset Overview\\n\\n\")\n",
    "    f.write(f\"- **Total Datasets:** {report['dataset_overview']['total_datasets']}\\n\")\n",
    "    f.write(f\"- **Total Records:** {report['dataset_overview']['total_records']:,}\\n\\n\")\n",
    "    \n",
    "    f.write(\"### Records per Dataset\\n\\n\")\n",
    "    f.write(\"| Dataset | Records |\\n\")\n",
    "    f.write(\"|---------|--------:|\\n\")\n",
    "    for name, count in report['dataset_overview']['datasets'].items():\n",
    "        f.write(f\"| {name} | {count:,} |\\n\")\n",
    "    \n",
    "    f.write(\"\\n## Identifier Mapping\\n\\n\")\n",
    "    f.write(\"| Identifier | Available In | Dataset Count |\\n\")\n",
    "    f.write(\"|------------|--------------|---------------:|\\n\")\n",
    "    for id_type, info in report['identifiers'].items():\n",
    "        datasets_str = ', '.join(info['available_in'])\n",
    "        f.write(f\"| {id_type} | {datasets_str} | {info['count']} |\\n\")\n",
    "    \n",
    "    f.write(\"\\n## Data Quality\\n\\n\")\n",
    "    f.write(\"| Dataset | Total Rows | Missing Values | Duplicates |\\n\")\n",
    "    f.write(\"|---------|------------|----------------|------------|\\n\")\n",
    "    for name, stats in report['data_quality'].items():\n",
    "        f.write(f\"| {name} | {stats['total_rows']:,} | {stats['missing_values']:,} | {stats['duplicates']:,} |\\n\")\n",
    "    \n",
    "    if report['temporal_coverage']:\n",
    "        f.write(\"\\n## Temporal Coverage\\n\\n\")\n",
    "        for dataset, coverage in report['temporal_coverage'].items():\n",
    "            f.write(f\"### {dataset.title()}\\n\")\n",
    "            f.write(f\"- Start: {coverage['start_date']}\\n\")\n",
    "            f.write(f\"- End: {coverage['end_date']}\\n\")\n",
    "            f.write(f\"- Duration: {coverage['duration_days']} days\\n\\n\")\n",
    "\n",
    "print(f\"‚úÖ Markdown report saved to: {md_report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c491e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Campus Data Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Dataset sizes\n",
    "if data:\n",
    "    dataset_sizes = {name: len(df) for name, df in data.items()}\n",
    "    axes[0, 0].bar(dataset_sizes.keys(), dataset_sizes.values())\n",
    "    axes[0, 0].set_title('Records per Dataset')\n",
    "    axes[0, 0].set_xlabel('Dataset')\n",
    "    axes[0, 0].set_ylabel('Number of Records')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Identifier availability\n",
    "if identifier_types:\n",
    "    id_counts = {k: len(v) for k, v in identifier_types.items() if v}\n",
    "    axes[0, 1].barh(list(id_counts.keys()), list(id_counts.values()))\n",
    "    axes[0, 1].set_title('Identifier Availability Across Datasets')\n",
    "    axes[0, 1].set_xlabel('Number of Datasets')\n",
    "\n",
    "# 3. Missing data percentage\n",
    "if 'profiles' in data:\n",
    "    df = data['profiles']\n",
    "    missing_pct = (df.isna().sum() / len(df)) * 100\n",
    "    missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "    if len(missing_pct) > 0:\n",
    "        axes[1, 0].barh(missing_pct.index, missing_pct.values, color='coral')\n",
    "        axes[1, 0].set_title('Missing Data in Profiles (%)')\n",
    "        axes[1, 0].set_xlabel('Percentage Missing')\n",
    "\n",
    "# 4. Temporal distribution (if swipes data available)\n",
    "if 'swipes' in data:\n",
    "    df = data['swipes'].copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    daily_counts = df.groupby('date').size()\n",
    "    axes[1, 1].plot(daily_counts.index, daily_counts.values)\n",
    "    axes[1, 1].set_title('Daily Swipe Activity')\n",
    "    axes[1, 1].set_xlabel('Date')\n",
    "    axes[1, 1].set_ylabel('Number of Swipes')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../data/analysis/dataset_overview.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved to: data/analysis/dataset_overview.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fazri-anal (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
